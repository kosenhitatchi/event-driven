<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>event-driven: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">event-driven
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">event-driven Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>| Read the <a href="http://robotology.github.io/event-driven/doxygen/doc/html/index.html">Documentation</a> | Download the <a href="https://github.com/robotology/event-driven">Code</a> |</p>
<h1>The event-driven YARP Project</h1>
<p><a href="https://youtu.be/xS-7xYRYSLc"></a> </p><div class="image">
<img src="http://robotology.github.io/event-driven/doxygen/images/ballstream.png"  alt="event-stream ball"/>
</div>
<p> Click to watch the <a href="https://youtu.be/xS-7xYRYSLc">video</a>!</p>
<p>Libraries that handle neuromorphic sensors, such as the dynamic vision sensor, installed on the iCub can be found here, along with algorithms to process the event-based data. Examples include, optical flow, corner detection and ball detection. Demo applications for the iCub robot, and tutorials for running them, include saccading and attention, gaze following a ball, and vergence control.</p>
<h2>Libraries</h2>
<p>Event-driven libraries provide basic functionality for handling events in a YARP environment. The library has definitions for:</p><ul>
<li>codecs to encode/decode events to be compatable with address event representation (AER) formats.</li>
<li>Sending packets of events in <code><a class="el" href="classev_1_1vBottle.html" title="yarp::os::Bottle wrapper for sending events through the yarp system with ensuring compatibility with ...">ev::vBottle</a></code> that is compatible with yarpdatadumper and yarpdataplayer.</li>
<li>asynchronous reading (<code><a class="el" href="classev_1_1queueAllocator.html" title="an asynchronous reading port that accepts vBottles and decodes them ">ev::queueAllocator</a></code>) and writing (<code><a class="el" href="classev_1_1collectorPort.html" title="an output port that can safely accept events from multiple threads and sends them at a fixed output r...">ev::collectorPort</a></code>) ports that ensure data is never dropped and giving access to delay information.</li>
<li>filters for removing salt and pepper noise.</li>
<li>event containers for organising the event-stream into temporal windows, fixed-size windows, surfaces, and regions-of-interest.</li>
<li>helper functions to handle event timestamp wrapping and to convert between timestamps and seconds.</li>
</ul>
<h2>Modules</h2>
<ul>
<li><b>Optical Flow</b> &ndash; an estimate of object velocity in the visual plane is given by the rate at which the spatial location of events change over time. Such a signal manifests as a manifold in the spatio-temporal event space. Local velocity can be extracted by fitting planes to these manifolds. The <code>ev::vFlow</code> module converts the ED camera output <code><a class="el" href="classev_1_1AddressEvent.html" title="an event with a pixel location, camera number and polarity ">ev::AddressEvent</a></code> to <code><a class="el" href="classev_1_1FlowEvent.html" title="an AddressEvent with a velocity in visual space ">ev::FlowEvent</a></code>.</li>
<li><b>Cluster Tracking</b> &ndash; The movement of an object across the visual field of an ED camera produces a detailed, unbroken trace of events. Local clusters of events can be tracked by updating a tracker position as new events are observed that belong to the same trace. The spatial distribution of the events can be estimated with a Gaussian distribution. The cluster centre and distribution statistics is output from the <code>ev::vCluster</code> module as a <code><a class="el" href="classev_1_1GaussianAE.html" title="a LabelledAE with parameters that define a 2D gaussian ">ev::GaussianAE</a></code> event.</li>
<li><b>Corner Detection</b> &ndash; using an event-driven Harris algorithm, the full event stream is filtered to contain only the events falling on the corners of objects or structure in the scene. Corner events are useful to avoid the aperture problem and to reduce the data stream to informative events for further processing. Compared to a traditional camera, the ED corner algorithm requires less processing. <code>ev::vCorner</code> converts <code><a class="el" href="classev_1_1AddressEvent.html" title="an event with a pixel location, camera number and polarity ">ev::AddressEvent</a></code> to <code><a class="el" href="classev_1_1LabelledAE.html" title="an AddressEvent with an ID or class label ">ev::LabelledAE</a></code>.</li>
<li><b>Circle Detection</b> &ndash; detection of circular shapes in the event stream can be performed using an ED Hough transform. As the camera moves on a robot, many background events clutter the detection algorithm. The <code>ev::vCircle</code> module reduces the false positive detections by using optical flow information to provide a more accurate understanding of only the most up-to-date spatial structure. <code>ev::vCircle</code> accepts <code><a class="el" href="classev_1_1AddressEvent.html" title="an event with a pixel location, camera number and polarity ">ev::AddressEvent</a></code> and <code><a class="el" href="classev_1_1FlowEvent.html" title="an AddressEvent with a velocity in visual space ">ev::FlowEvent</a></code> and outputs <code><a class="el" href="classev_1_1GaussianAE.html" title="a LabelledAE with parameters that define a 2D gaussian ">ev::GaussianAE</a></code>.</li>
<li><b>Particle filtering</b> &ndash; probabilistic filtering is used to provide a robust tracking over time. The particle filter is robust to variations in speed of the target by also sampling within the temporal dimension. A observation likelihood function that responds to a circular shape was developed to instigate a comparison with the Hough transform. The tracking position is output as <code><a class="el" href="classev_1_1GaussianAE.html" title="a LabelledAE with parameters that define a 2D gaussian ">ev::GaussianAE</a></code>. Future work involves adapting the filter to respond to different target shapes, and templates learned from data.</li>
</ul>
<h2>Applications for the iCub Humanoid Robot</h2>
<p>Tutorials for these applications can be found <a href="http://robotology.github.io/event-driven/doxygen/doc/html/pages.html">here</a></p>
<ul>
<li>viewing the event-stream in 3D spatio-temporal space.</li>
<li>calibrating the event-camera with a static fiducial.</li>
<li>performing saccading and attention.</li>
<li>ball detection and iCub gaze following.</li>
<li>performing automatic stereo vergence.</li>
</ul>
<p>Datasets for use in running some of the tutorials off-line can be found on the same page.</p>
<h2>How to Install:</h2>
<p>Comprehensive instructions available if you are a first-time user of YARP <a href="http://robotology.github.io/event-driven/doxygen/doc/html/pages.html">here</a>.</p>
<p>Quick instructions:</p>
<ol type="1">
<li>Install <a href="https://github.com/robotology/yarp">YARP</a> and <a href="https://github.com/robotology/icub-contrib-common">icub-contrib-common</a> following these <a href="http://wiki.icub.org/wiki/Linux:Installation_from_sources">instructions</a>.</li>
<li>git clone <a href="https://github.com/robotology/event-driven.git">https://github.com/robotology/event-driven.git</a></li>
<li>cd event-driven</li>
<li>mkdir build &amp;&amp; cd build</li>
<li>ccmake ..</li>
<li>ensure the install path is as configured in icub-contrib-common</li>
<li>turn on desired modules and applications (e.g. processing)</li>
<li>configure (c) and generate (g)</li>
<li>make install</li>
</ol>
<h2>References</h2>
<p>Glover, A., and Bartolozzi C. (2016) <em>Event-driven ball detection and gaze fixation in clutter</em>. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 2016, Daejeon, Korea. <b>Finalist for RoboCup Best Paper Award</b></p>
<p>Vasco V., Glover A., and Bartolozzi C. (2016) <em>Fast event-based harris corner detection exploiting the advantages of event-driven cameras</em>. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 2016, Daejeon, Korea.</p>
<p>V. Vasco, A. Glover, Y. Tirupachuri, F. Solari, M. Chessa, and Bartolozzi C. <em>Vergence control with a neuromorphic iCub. In IEEE-RAS International Conference on Humanoid Robots (Humanoids)</em>, November 2016, Mexico. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Thu Dec 14 2017 13:47:09 for event-driven by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
